---
title: PA Lidar wall follow
desc: Using LIDAR sensing to drive the robot along a wall
slides: true
---
:slide_title Purpose

Wall following is a classic early Robotics problem to solve. It brings together motion control and sesning with Lidar. It is a building block for some of the next assignments as well. I think you will find it quite interesting..

:slide_title Skills that you will practice

* ROS App: Basic structure of a ros app, ROSCore and Nodes
* launch and run: Launch files, bringing up robots, running with simulation
* Motion: Control motion with cmd_vel
* Pubsub: publish and subscribe
* sensing: sensing with lidar and odom
* calc: Algorithms, calculations and state management
* Debug: Finish, debug, and get an app to work

:slide_title Assignment Specifics

* At a high level, you are writing a node which will move the robot along a wall, maintaining a fixed distance.
* Test and demonstrate the code in simulation
* Consider the following basic starting  conditions:
  * Robot already at the right distance from the wall and pointing parallel to the wall
  * Robot at 1.5 the desired distance and pointing in the right direction
  * Robot at 0.5 the desired distance and pointing in the right direction
* These are some more challenging starting conditions:
  * Robot is pointing in various other ways
  * Robot is well away from the wall
  * The wall has an inside corner
  * The wall has an outside corner
* Beware: It will be hard to get all of these to work correctly but it is worth the struggle
* Don't pressure yourself to get all of them to work seamlessly.
* We will assess this based on how well the simple cases work and how far your attempts at the more difficult cases get.

:slide_title How to Run ([Sample Solution](https://www.youtube.com/watch?v=yH474O4mAdw&list=PLWp7_Yk4l1aPcMGxCCvqKCSwnkTBBInI3&index=76))

1. `roslaunch turtlebot3_gazebo turtlebot3_stage_1.launch`
2. `roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch`
2. Put the robot where you want it to start
3. `rosrun wall_follower wall_follower.py`

:slide_bigtitle Tips and thought process

At the highest level the algorithm is a loop that:

1. Determines the distance to the wall (e.g. on the left)
1. Tells the robot to move and/or turn to get to and maintain the desired distance.

To do this it will publish to cmd_vel to order movement of the robot, and subscribe to /scan to get data from the lidar. You will process the Lidar data to figure out where the robot is relative to the wall.

:slide_title Processing Lidar Data

* Remember that the Lidar will generate 10 or more "scans" per second. and each time your callback will be called.
* Typically 0 degrees is foreward. and then we go counter clockwise around positive up to 359
* Typically we analyze the data by dividing the 360 degrees around the robot into "wedges", for example, forward might be from -15 to +15 degrees. Left might be from 250  to 290 degrees
* The data from the Lidar needs a little cleaning. Any Inf or NaN values should be ignored. Any values outside of the spec range of the Lidar (e.g below 0.1 meter or above 5m) should also be ignored.
* Think through how when the robot is not aligned paralel to the wall (e.g lets say it is pointing at it) your LIDAR will not report a wall to the left, even if it is very close to it

:slide_title How to think about the algorithm

* Theh logic is basically a loop where you are looking at data derived from the LIDAR and deciding what cmd_vel to publish. If the robot is at the right distance and pointing in the right direction then you command it to drive straight.
* But if it's too far, how do you get it to get closer? A combination of linear velocity with a little angular. But how much? Do you give more angular velocity if you are farther? But then the robot may end up pointing in the wrong direction so that your measurements of the distance turn out to be wrong.

:slide_title Good Engineering Practices

* Avoid "magic numbers". Define them in some way. Simplest is an (all caps) global Python variable, but there are other ways.
* Use print statements to make the behavior of your code more transparent to you.
* Aternatively write print statements which print out a list of numbers, separated by commas. That way you can capture those in a file, load them into a spreadsheet or script and look at the data visually
* Learn to use the python debugger
* Remember YAGNI - You're not gonna need it. Implement and test one case before going to the more complicated case. For example, the case where the robot is pointing paralel to the wall and at the right distance. Can you get it to maintain that. Once you have that, try it when the robot is too far. Then not eactly parlel. 

:slide_title Practical Tips

* Put print statements to let you see a log of actions while you are debugging
* Get a feel for what kind of data the `scan` topic publishes by moving the robot around in Gazebo and executing `rostopic echo scan` on a separate terminal.
* Change the `fixed_frame` in RViz from `odom` to `base_link` to make RViz graphically display the LIDAR readings.
* Typically, forward motion are positive cmd_vel linear.x and counter clockwise rotation is positive angular.z.

:slide_title Resources and Tips

I recommend you look at all of these!

* Reference: :topic_link :working_with_lidar_data
* Reference: [How to move objects in Gazebo using its GUI](https://answers.gazebosim.org//question/13445/how-to-move-objects-and-robot-models-using-gui/)
* Reference: [Difference between Gazebo and RViz](https://answers.ros.org/question/200044/different-between-gazebo-and-rviz/)
* Reference: [Udacity AI for Robotics Course article](https://classroom.udacity.com/courses/cs373/lessons/48684821)
* Reference: [A wall-following algorithm based on dynamic virtual walls for mobile robots navigation](/content/topics/documents/wallfollower.pdf)


:slide_title Deliverables
* ROS Package with
  * Commented Python source code file
  * README with specific instructions on how to run it
  * Correct package.xml CMakeLists.txt
* Video of your program running in gazebo or the lab
*Filled in by Ephraim (ezimmerman@brandeis.edu for questions)*

Please follow the following steps for a successful submission: 

Video submission: 

1. Record your video submission.
2. Upload the video file to Google Drive and ensure that "Anyone with the link" is selected on the sharing tab. 
3. Copy/paste this link as an inline comment below the import statements in the wall_follower.py file. DO NOT remove the `#!/usr/bin/env python3` present on the first line of the code.  

Code submission: 

1. Navigate to `/my_ros_data/catkin_ws/src/cosi119_src/wall_follower/src` in your cluster account VSCode view.
2. Compress the files: `zip basic_mover_<brandeis email prefix>.zip wall_follower.py`. Replace <brandeis prefix> with your email prefix. For example, ezimmerman.
3. The previous step will generate a .zip file. Right click it, then press download. This will download the files onto your machine. 
4. Upload the zip file, named, for example, wall_follower_ezimmerman.zip, to the Moodle submission.



