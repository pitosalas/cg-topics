---
title: Seed Data
---
* To standardize the scalability tests, use this set of seed data
* You can find it here: [Nick's Super Secret Seed Data](https://www.dropbox.com/s/3xpzbnns2445wa8/seeds.zip?dl=0)

### Files
* There are three csv files:
  * user.csv is id,name where user with id has name
  * tweets.csv is id,tweet where user with id posted tweet
  * follows.csv is id1,id2 where user with id1 follows user with id2

### Content
* There are 1k users.
* Each user has 0-200 tweets (average 100)
* Each user follows (0-10) others
* No user follows itself.
* No two users have the same name.  Tweet text could be repeated.

### How to use
* This is the bare-bones for required fields
* Each of your schemas will be a little different 
* So you will need to process these files
* Feel free to add any additional fields with Faker.

### Loading the data
* You will need to write code to read in and hook up this data into your nanoTwitter
* Simply adding one record at a time will be unbelievably slow (scalability!)
* There have been many gems to address this and make it faster. You can search for them
* But with Rails 6, there's actually an offical way to bulk insert: [Bulk Insert Support in Rails 6](https://blog.bigbinary.com/2019/04/15/bulk-insert-support-in-rails-6.html)
* Take a good look at :topic_link :nt_test_interface / to see how the data loading is requested via the :topic_link :nt_test_interface /

